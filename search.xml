<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>transformer</title>
      <link href="/2023/11/12/transformer/"/>
      <url>/2023/11/12/transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="传统RNN模型的弊端"><a href="#传统RNN模型的弊端" class="headerlink" title="传统RNN模型的弊端"></a>传统RNN模型的弊端</h1><p><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699788663619-07d93166-3458-4227-a2ce-9eb90e47523e.png#averageHue=%23d8e5ca&clientId=u5096c23f-72d6-4&from=paste&height=568&id=ud0b7d75e&originHeight=774&originWidth=2373&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=734864&status=done&style=none&taskId=u4ca415f7-3421-4e89-ad05-ed020cb8f2b&title=&width=1740.3207846117975" alt="image.png"><br />传统RNN模型在计算的时候需要利用到之前的数据，即X0计算h0后的中间变量会被X1计算h1的时候使用。无法并行计算<br><a name="rMXaB"></a></p><h1 id="self-attention的计算方式"><a href="#self-attention的计算方式" class="headerlink" title="self-attention的计算方式"></a>self-attention的计算方式</h1><p><a name="WBbkj"></a></p><h2 id="数据编码和qkv初始化"><a href="#数据编码和qkv初始化" class="headerlink" title="数据编码和qkv初始化"></a>数据编码和qkv初始化</h2><p><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699788826015-9b1f0c77-0a02-489e-9f1d-d308abf40fb6.png#averageHue=%23fefdfc&clientId=u5096c23f-72d6-4&from=paste&height=154&id=qbvyC&originHeight=858&originWidth=1424&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=253840&status=done&style=none&taskId=u54d36042-9a5c-413f-963c-4e0cbed124b&title=&width=255.50497436523438" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699792667745-376912eb-ef7e-44c5-8e92-82ab249a0340.png#averageHue=%23fefcfb&clientId=u5096c23f-72d6-4&from=paste&height=161&id=u34f835df&originHeight=1107&originWidth=983&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=295957&status=done&style=none&taskId=u30841d5b-0c1a-496a-b4c0-0dc72037147&title=&width=142.91671752929688" alt="image.png"><br />X1、X2是编码的数据，编码方式因方式而异，是将要处理的数据抽象成为理想的数据。<br />左图中X<em>W&#x3D;q&#x2F;k&#x2F;v，可以观察到例子中的X是1</em>4的矩阵×4<em>3的矩阵就能够得到1</em>3的目标q、k、v。<br />右图就是将多个x数据合并为一个X同时进行计算之后的结果。<br />数据含义：<br /><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699792940512-c0b84ffd-6c18-4a07-9c3c-e2e5cfdf6a0c.png#averageHue=%23f8f8f8&clientId=u5096c23f-72d6-4&from=paste&height=180&id=u5ccdfeff&originHeight=591&originWidth=1043&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=177198&status=done&style=none&taskId=ud39a9398-5bf1-4e96-b4e4-927b6d02d91&title=&width=318.52789306640625" alt="image.png"><br><a name="qjmGr"></a></p><h2 id="注意力得分（attention-score）"><a href="#注意力得分（attention-score）" class="headerlink" title="注意力得分（attention score）"></a>注意力得分（attention score）</h2><p><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699793153334-ae8fde05-a0b5-4097-8a9e-a3fad064d204.png#averageHue=%23fbfaf9&clientId=u5096c23f-72d6-4&from=paste&height=221&id=u38e235af&originHeight=811&originWidth=2249&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=602149&status=done&style=none&taskId=uea7cdd9a-fba6-4e6d-8cdc-3eb608aae3a&title=&width=613.5194702148438" alt="image.png"><br />q1分别与各个k进行内积（内积：线性无关-&gt;内积为0，相互垂直，相关性越大内积越大）得到的是对应输出b1的注意力得分。<br /><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699793354205-7402d190-c785-414d-b9a8-442890c32c32.png#averageHue=%23f8f7f5&clientId=u5096c23f-72d6-4&from=paste&height=275&id=u0694cb38&originHeight=375&originWidth=692&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=48599&status=done&style=none&taskId=uf748e162-290a-4da6-9b99-7e1db6cab05&title=&width=507.5018891493316" alt="4a7cf245a4d34dd893afe09545574116.png"><br><a name="N1OFU"></a></p><h2 id="最终得分（softmax）"><a href="#最终得分（softmax）" class="headerlink" title="最终得分（softmax）"></a>最终得分（softmax）</h2><p>希望得到最终的数据得分还需要一步softmax操作<br /><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699794993476-5e34d3d5-8046-465d-b715-22cd174066aa.png#averageHue=%23fdfdfc&clientId=u5096c23f-72d6-4&from=paste&height=133&id=u688b92d1&originHeight=635&originWidth=1208&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=151501&status=done&style=none&taskId=uc3b5ee1d-1f96-46fd-a67c-431503261c8&title=&width=253.51947021484375" alt="image.png"><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699794486256-2c876a25-0742-4c69-99b4-ad93acb038ce.png#averageHue=%23e1e7dc&clientId=u5096c23f-72d6-4&from=paste&height=126&id=Klsu6&originHeight=368&originWidth=1032&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=213766&status=done&style=none&taskId=u036b869a-18ef-4cf6-baf2-880a60208aa&title=&width=353.5248107910156" alt="image.png"><br /><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699794350091-330df45d-29ba-4df0-a36d-2536b31d60fb.png#averageHue=%23f6dea0&clientId=u5096c23f-72d6-4&from=paste&height=344&id=u08d042c2&originHeight=712&originWidth=961&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=306867&status=done&style=none&taskId=ufcdcb35e-058f-4c4c-a427-1fd9c12f426&title=&width=463.775390625" alt="image.png"><br />Softmax是一种<strong>数学函数</strong>，通常用于将一组任意实数转换为表示<strong>概率分布</strong>的实数。其本质上是一种<strong>归一化函数</strong>，可以将一组<strong>任意的实数值转化为在[0, 1]之间的概率值</strong>，因为softmax将它们转换为0到1之间的值，所以它们可以被解释为概率。如果其中一个输入很小或为负，softmax将其变为小概率，如果输入很大，则将其变为大概率，但它将始终保持在0到1之间。  <br />Softmax是<strong>逻辑回归</strong>的一种<strong>推广</strong>，可以用于<strong>多分类任务</strong>，其公式<strong>与逻辑回归的sigmoid函数非常相似</strong>。只有当分类是互斥的，才可以在分类器中使用softmax函数，也就是说只能是多元分类（即数据只有一个标签），而不能是多标签分类（即一条数据可能有多个标签）。  <br />许多多层神经网络输出层的最后一层是一个<strong>全连接层</strong>，输出是一个实数向量，这个向量通常代表了<strong>每个类别的得分或置信度</strong>。为了将这些得分转换为<strong>概率分布</strong>，通常会使用softmax函数。因为它将分数转换为规范化的概率分布，可以显示给用户或用作其他系统的输入。所以通常附加一个softmax函数在神经网络的最后一层之后。  <br /><img src="https://cdn.nlark.com/yuque/0/2023/jpeg/22969703/1699794051685-7dfd90ca-fe81-422e-8ed0-d139896eddb6.jpeg#averageHue=%23f3f3f3&clientId=u5096c23f-72d6-4&from=paste&height=177&id=u25209112&originHeight=355&originWidth=586&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=14888&status=done&style=none&taskId=u40541f54-55bc-40f6-9995-e32cff0ffa0&title=&width=291.76318359375" alt="v2-b4f1deff538c4ede8ac7fd568af4570b_1440w.jpg"><br /><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699794213557-88effa2c-3cf6-4e11-8890-a60a89bbd19e.png#averageHue=%23e2cbac&clientId=u5096c23f-72d6-4&from=paste&height=352&id=u944e622a&originHeight=1119&originWidth=1637&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=798837&status=done&style=none&taskId=u6a5eab27-c835-4123-b337-da5dca8c2c7&title=&width=515.5278930664062" alt="image.png"><br><a name="wl65s"></a></p><h2 id="整个流程的数据实例"><a href="#整个流程的数据实例" class="headerlink" title="整个流程的数据实例"></a>整个流程的数据实例</h2><p><img src="https://cdn.nlark.com/yuque/0/2023/png/22969703/1699797881176-64f80b4a-ef27-4996-bf00-324910ebd1c3.png#averageHue=%23fdfcfb&clientId=u5808d3df-9eb2-4&from=paste&height=304&id=uba2910c8&originHeight=606&originWidth=576&originalType=url&ratio=1.3635417222976685&rotation=0&showTitle=false&status=done&style=none&taskId=u4e4b621d-5f6d-439b-9bc6-ea2e1d1eeaf&title=&width=288.99310302734375"><br />可以看到正如前文所说的，最终得到的b1也就是图中的z1，即x1对应的输出数据是与q1与所有k的内积有关，内积&#x2F;根号d的softmax对应的是V1….n的分别的权值，利用权值和实际的数值就能计算出最终的结果z。<br><a name="mrWDR"></a></p><h2 id="self-attention与scaled-Dot-Product-Attention"><a href="#self-attention与scaled-Dot-Product-Attention" class="headerlink" title="self-attention与scaled Dot-Product Attention"></a>self-attention与scaled Dot-Product Attention</h2><p><img src="https://cdn.nlark.com/yuque/0/2023/webp/22969703/1699798375819-dfe1e595-0a33-4837-86a7-4607121ce09d.webp#averageHue=%23f7f7f6&clientId=u5808d3df-9eb2-4&from=paste&height=358&id=u0f9bd492&originHeight=488&originWidth=406&originalType=binary&ratio=1.3635417222976685&rotation=0&showTitle=false&size=9876&status=done&style=none&taskId=u9714415f-4e02-4dc4-8862-f8f0a02f9de&title=&width=297.7539985471512" alt="v2-3b24f9497d010008e7603ec5584fe0a6_1440w.webp"></p><blockquote><p>上述attention可以被描述为将query和key-value键值对的一组集合映到输出，其中 query，keys，values和输出都是向量，输出被计算为values的加权和，其中分配给每个value的权重由query与对应key的相似性函数计算得来。这种attention的形式被称为“<strong>Scaled Dot-Product Attention</strong>”</p></blockquote><p>  也就是说，这种利用相互之间的关联关系作为权值求最终key的形式称为<strong>Scaled Dot-Product Attention</strong>，而self-attention是一个流程，之中用到了这种方法。<br><a name="UZXPO"></a></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://www.bilibili.com/video/BV1ED4y1872X?p=5&spm_id_from=pageDriver&vd_source=8e0c4a056d693380e8f9054702672766">4-self-attention计算方法1.mp4_哔哩哔哩_bilibili</a><br /><a href="https://blog.csdn.net/Michale_L/article/details/126549946?app_version=6.1.7&csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22126549946%22,%22source%22:%22weixin_65447388%22%7D&utm_source=app">自注意力机制(Self-Attention)_Michael_Lzy的博客-CSDN博客</a><br /><a href="https://zhuanlan.zhihu.com/p/628492966">Softmax简介</a><br /><a href="https://www.bilibili.com/video/BV18u411L7tU/?spm_id_from=333.337.search-card.all.click&vd_source=8e0c4a056d693380e8f9054702672766">什么是softmax回归，如何使用softmax回归，解决多分类任务_哔哩哔哩_bilibili</a><br /><a href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=8e0c4a056d693380e8f9054702672766">Transformer中Self-Attention以及Multi-Head Attention详解_哔哩哔哩_bilibili</a><br /><a href="https://blog.csdn.net/zwqjoy/article/details/121698660">[深度学习] 自然语言处理—Transformer原理(一)_self attention 和scaled dot-product attention_舒克与贝克的博客-CSDN博客</a><br /><a href="https://zhuanlan.zhihu.com/p/542312699?utm_id=0">【自然语言处理】1. 细讲：Attention模型的机制原理</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>vscode学习</title>
      <link href="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="创建工程"><a href="#创建工程" class="headerlink" title="创建工程"></a>创建工程</h1><p>参考文献：</p><p><a href="https://www.cnblogs.com/englyf/p/16685082.html">https://www.cnblogs.com/englyf/p/16685082.html</a></p><p>这里我的vscode是英文，windows系统中我没改</p><p>这里很有意思，vscode是自己创建个文件夹然后打开这个文件夹</p><img src="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/1679196530859-25605dbc-b9d4-41c9-b1f7-49a0600aec62.png" class="" title="img"><p>自己创建个cpp文件就行了，左边栏里面有文件+号的标志</p><img src="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/1679196759569-22cefc6a-442b-40c4-b51a-e5986f5e7ab5.png" class="" title="img">]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
