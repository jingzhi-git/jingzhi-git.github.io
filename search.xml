<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>transformer</title>
      <link href="/2023/11/12/transformer/"/>
      <url>/2023/11/12/transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="传统RNN模型的弊端"><a href="#传统RNN模型的弊端" class="headerlink" title="传统RNN模型的弊端"></a>传统RNN模型的弊端</h1><img src="/2023/11/12/transformer/1699788663619-07d93166-3458-4227-a2ce-9eb90e47523e.png" class="" title="img"><br />传统RNN模型在计算的时候需要利用到之前的数据，即X0计算h0后的中间变量会被X1计算h1的时候使用。无法并行计算。<p><a name="rMXaB"></a></p><h1 id="self-attention的计算方式"><a href="#self-attention的计算方式" class="headerlink" title="self-attention的计算方式"></a>self-attention的计算方式</h1><p><a name="WBbkj"></a></p><h2 id="数据编码和qkv初始化"><a href="#数据编码和qkv初始化" class="headerlink" title="数据编码和qkv初始化"></a>数据编码和qkv初始化</h2><img src="/2023/11/12/transformer/1699788826015-9b1f0c77-0a02-489e-9f1d-d308abf40fb6.png" class="" title="img"><img src="/2023/11/12/transformer/1699792667745-376912eb-ef7e-44c5-8e92-82ab249a0340.png" class="" title="img"><br />X1、X2是编码的数据，编码方式因方式而异，是将要处理的数据抽象成为理想的数据。<br />左图中X*W=q/k/v，可以观察到例子中的X是1*4的矩阵×4*3的矩阵就能够得到1*3的目标q、k、v。<br />右图就是将多个x数据合并为一个X同时进行计算之后的结果。<br />数据含义：<br /><img src="/2023/11/12/transformer/1699792940512-c0b84ffd-6c18-4a07-9c3c-e2e5cfdf6a0c.png" class="" title="img"><p><a name="qjmGr"></a></p><h2 id="注意力得分（attention-score）"><a href="#注意力得分（attention-score）" class="headerlink" title="注意力得分（attention score）"></a>注意力得分（attention score）</h2><p>!<img src="/2023/11/12/transformer/1699793153334-ae8fde05-a0b5-4097-8a9e-a3fad064d204.png" class="" title="img"><br />q1分别与各个k进行内积（内积：线性无关-&gt;内积为0，相互垂直，相关性越大内积越大）得到的是对应输出b1的注意力得分。<br /><img src="/2023/11/12/transformer/1699793354205-7402d190-c785-414d-b9a8-442890c32c32.png" class="" title="4a7cf245a4d34dd893afe09545574116.png"><br><a name="N1OFU"></a></p><h2 id="最终得分（softmax）"><a href="#最终得分（softmax）" class="headerlink" title="最终得分（softmax）"></a>最终得分（softmax）</h2><p>希望得到最终的数据得分还需要一步softmax操作<br /><img src="/2023/11/12/transformer/1699794993476-5e34d3d5-8046-465d-b715-22cd174066aa.png" class="" title="img"><img src="/2023/11/12/transformer/1699794486256-2c876a25-0742-4c69-99b4-ad93acb038ce.png" class="" title="image.png"><br /><img src="/2023/11/12/transformer/1699794350091-330df45d-29ba-4df0-a36d-2536b31d60fb.png" class="" title="image.png"><br />Softmax是一种<strong>数学函数</strong>，通常用于将一组任意实数转换为表示<strong>概率分布</strong>的实数。其本质上是一种<strong>归一化函数</strong>，可以将一组<strong>任意的实数值转化为在[0, 1]之间的概率值</strong>，因为softmax将它们转换为0到1之间的值，所以它们可以被解释为概率。如果其中一个输入很小或为负，softmax将其变为小概率，如果输入很大，则将其变为大概率，但它将始终保持在0到1之间。  <br />Softmax是<strong>逻辑回归</strong>的一种<strong>推广</strong>，可以用于<strong>多分类任务</strong>，其公式<strong>与逻辑回归的sigmoid函数非常相似</strong>。只有当分类是互斥的，才可以在分类器中使用softmax函数，也就是说只能是多元分类（即数据只有一个标签），而不能是多标签分类（即一条数据可能有多个标签）。  <br />许多多层神经网络输出层的最后一层是一个<strong>全连接层</strong>，输出是一个实数向量，这个向量通常代表了<strong>每个类别的得分或置信度</strong>。为了将这些得分转换为<strong>概率分布</strong>，通常会使用softmax函数。因为它将分数转换为规范化的概率分布，可以显示给用户或用作其他系统的输入。所以通常附加一个softmax函数在神经网络的最后一层之后。  <br /><img src="/2023/11/12/transformer/1699794051685-7dfd90ca-fe81-422e-8ed0-d139896eddb6.jpeg" class="" title="v2-b4f1deff538c4ede8ac7fd568af4570b_1440w.jpg"><br /><img src="/2023/11/12/transformer/1699794213557-88effa2c-3cf6-4e11-8890-a60a89bbd19e.png" class="" title="img"><br><a name="wl65s"></a></p><h2 id="整个流程的数据实例"><a href="#整个流程的数据实例" class="headerlink" title="整个流程的数据实例"></a>整个流程的数据实例</h2><img src="/2023/11/12/transformer/1699797881176-64f80b4a-ef27-4996-bf00-324910ebd1c3.png" class="" title="img"><br />可以看到正如前文所说的，最终得到的b1也就是图中的z1，即x1对应的输出数据是与q1与所有k的内积有关，内积/根号d的softmax对应的是V1....n的分别的权值，利用权值和实际的数值就能计算出最终的结果z。<p><a name="mrWDR"></a></p><h2 id="self-attention与scaled-Dot-Product-Attention"><a href="#self-attention与scaled-Dot-Product-Attention" class="headerlink" title="self-attention与scaled Dot-Product Attention"></a>self-attention与scaled Dot-Product Attention</h2><img src="/2023/11/12/transformer/1699798375819-dfe1e595-0a33-4837-86a7-4607121ce09d.webp" class="" title="v2-3b24f9497d010008e7603ec5584fe0a6_1440w.webp"><blockquote><p>上述attention可以被描述为将query和key-value键值对的一组集合映到输出，其中 query，keys，values和输出都是向量，输出被计算为values的加权和，其中分配给每个value的权重由query与对应key的相似性函数计算得来。这种attention的形式被称为“<strong>Scaled Dot-Product Attention</strong>”</p></blockquote><p>  也就是说，这种利用相互之间的关联关系作为权值求最终key的形式称为<strong>Scaled Dot-Product Attention</strong>，而self-attention是一个流程，之中用到了这种方法。<br><a name="UZXPO"></a></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://www.bilibili.com/video/BV1ED4y1872X?p=5&spm_id_from=pageDriver&vd_source=8e0c4a056d693380e8f9054702672766">4-self-attention计算方法1.mp4_哔哩哔哩_bilibili</a><br /><a href="https://blog.csdn.net/Michale_L/article/details/126549946?app_version=6.1.7&csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22126549946%22,%22source%22:%22weixin_65447388%22%7D&utm_source=app">自注意力机制(Self-Attention)_Michael_Lzy的博客-CSDN博客</a><br /><a href="https://zhuanlan.zhihu.com/p/628492966">Softmax简介</a><br /><a href="https://www.bilibili.com/video/BV18u411L7tU/?spm_id_from=333.337.search-card.all.click&vd_source=8e0c4a056d693380e8f9054702672766">什么是softmax回归，如何使用softmax回归，解决多分类任务_哔哩哔哩_bilibili</a><br /><a href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=8e0c4a056d693380e8f9054702672766">Transformer中Self-Attention以及Multi-Head Attention详解_哔哩哔哩_bilibili</a><br /><a href="https://blog.csdn.net/zwqjoy/article/details/121698660">[深度学习] 自然语言处理—Transformer原理(一)_self attention 和scaled dot-product attention_舒克与贝克的博客-CSDN博客</a><br /><a href="https://zhuanlan.zhihu.com/p/542312699?utm_id=0">【自然语言处理】1. 细讲：Attention模型的机制原理</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>vscode学习</title>
      <link href="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="创建工程"><a href="#创建工程" class="headerlink" title="创建工程"></a>创建工程</h1><p>参考文献：</p><p><a href="https://www.cnblogs.com/englyf/p/16685082.html">https://www.cnblogs.com/englyf/p/16685082.html</a></p><p>这里我的vscode是英文，windows系统中我没改</p><p>这里很有意思，vscode是自己创建个文件夹然后打开这个文件夹</p><img src="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/1679196530859-25605dbc-b9d4-41c9-b1f7-49a0600aec62.png" class="" title="img"><p>自己创建个cpp文件就行了，左边栏里面有文件+号的标志</p><img src="/2023/03/19/vscode%E5%AD%A6%E4%B9%A0/1679196759569-22cefc6a-442b-40c4-b51a-e5986f5e7ab5.png" class="" title="img">]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
